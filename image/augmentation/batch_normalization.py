# Batch Normalization
Deep Learningにおける各重みパラメータを上手くreparametrizationすることで、ネットワークを最適化するための方法の一つ
各ユニットの出力をminibatchごとにnormalizeした新たな値で置き直すことで、内部の変数の分布(内部共変量シフト)が大きく変わるのを防ぎ、
学習が早くなる、過学習が抑えられるなどの効果が得られる

共変量シフトというのは、こちらで書かれている通り、データの分布が訓練時と推定時で異なるような状態のことを言う
訓練中にネットワーク内の各層の間で起きる共変量シフトを内部共変量シフトと言う